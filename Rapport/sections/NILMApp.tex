\chapter{NILM As An Application} 
\label{sec:CaseStudy}
The last couple of years have the number of smart-meters installed in residential houses increased drastically~\citep{RefWorks:44},~\citep{RefWorks:45}. The motivation for installing smart meters in the different houses have been to better understand energy consumption, in order to better plan energy distribution and production, and increase security~\citep{RefWorks:43}. This on of steps in creating a modern smart grid. With the smart grid came an opportunity for \ab{NILM}, since the equipment and infrastructure needed to measure the consumption and transfer the results through the Internet will be available in many households. 

The applications proposed in many of the articles published about \ab{NILM} focuses on energy management. Either it is for the electricity producers, that needs it to better predict consump-tion, or it is for the resident of the house that can optimize power usage to obtain savings \citep{RefWorks:17}. Even though these are fine examples of the potential usages of \ab{NILM} there might also exist an opportunity for the \dfs{service provider} in the smart grid to sell the \ab{NILM} information. An instance that collect information with the purpose of selling it is defined as a \df{data broker}. This chapter contains a small case study to illustrate how a \df{service provider} could obtain TV information from a city and act as a \df{data broker} to a local TV station. 

\section{The TV Viewing Habits Case}
The case setting is a small town, which have a number of households and an electricity utility company supplying energy to the city. For this case data from the SmartHG dataset is used to simulate a small town with 4 houses. The houses selected for this case are 10, 13, 18 and 23  since they contain a TV that are relatively dominant, as discussed in Section~\ref{sec:MMIRTSM}. 

\begin{figure}[H]
\begin{picture}(0,150)
\put(100,0){\includegraphics[width=0.6\textwidth]{billeder/CaseIlu.png}}

\put(177,110){Network}
\put(316,85){Service}
\put(312,75){Provider}

\put(319,0){Utility}
\put(314,-10){Company}

\put(122,62){\color{white} \textbf{10}}
\put(174,62){\color{white} \textbf{13}}
\put(226,62){\color{white} \textbf{18}}
\put(276,62){\color{white} \textbf{23}}
\put(200,0){Power Line}

\end{picture}
\caption{The city setup}
\label{fig:CaseSetup}
\end{figure}

The city setup is illustrated in Figure~\ref{fig:CaseSetup}. The figure illustrates how the utility company have a \df{generator} role and supplies energy to the houses. All the houses have smart meters, and are informing the utility company and the \dfs{service provider} about the current consumption of each house. This is done using some network. This is a fairly simplified example of a smart grid, where only the \df{customer}, \df{generator} and \df{service provider} roles are used. The power line is illustrated as a single feeder, but could just as well be a complex \df{distribution} network. In this case is it assumed that the \df{customer} sends information about the consumption of the houses every 30 seconds, as in the SmartHG dataset. Even though many smart-meters today are capable of delivering information at this speed, it is more common to only get information each hour. This is mainly because the information is used to regulate distribution and production, which is a slow process that would not improve by faster update rates.  

The \dfs{service provider} collect the information from the smart meters, and by using \ab{NILM} can calculate statistical information about the town. In the fictive case is a local TV station interested in knowing what time a day the citizens of the town watches TV, in order to improve some aspect of their product. 

\section{Methodology}
The aim of this case is to illustrate the potential of this kind of application for a \df{service provider}. Therefore is the data analysis conducted in a manner, that illustrates how a \df{service provider} could do it. This means that only the main meter information is used in the analysis, and the sub-meter information is only used to validate the results.  The goal is for the \df{service provider} to disaggregate the TV signals from the main meter signal, in order to use it in some statistical analysis. 

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{billeder/Electric company method.png}
\caption{The disaggregation process}
\label{fig:ECM}
\end{figure}

This process is illustrated in Figure~\ref{fig:ECM}. It is assumed that the \dfs{service provider} have access to a very general statistical model of the TV. This model can come from some outside database, or be one that the utility company or \df{service provider} developed themselves. The first step is to create a household specific model, which is a specialisation of the general model. This could be done with sub-meter data. To make the experiment more realistic is the training of the statistical disaggregation models done using only the accumulated data from the main meter. This is done using the same method as developed for the Parson algorithm \citep{RefWorks:28}. This method finds periods where an appliance is turned ON and OFF without any other appliances changing states, is detected. These ON/OFF single-event fragments are then mapped to specific appliances using the general models. When enough ON/OFF single-event fragments have been collected, can this information be used to train a more specific model. This can happen in a contentiously manner where the specific model is improved as more data is collected.  

Using the specific models can disaggregation of the main meter signal be achieved. The statistical disaggregation models selected in this case are found using the \ab{FHMM} algorithm. As discussed in Section~\ref{sec:NOISE}, the disaggregation alone gives a very error prone signal. The signal is therefore feed through a \df{norm filter}. As discussed in Section~\ref{sec:NormFilter} can the information about how a user normally would use the appliance be used to filter the disaggregated data to obtain better results. 

The result of this process is an appliance usage pattern, that can be used by the \dfs{service provider} to derive various statistical properties.  

The experiment is conducted with four weeks of data been used to train TV specific models for each house. The analysis for the TV station is conducted over a period of six weeks that is not overlapping with the initial four training weeks. 

\section{Results}
The viewing habits of the small town obtained by the disaggregation analysis is compared with the actual viewing habit obtained from the sub-meter data. In Figure~\ref{fig:WHW} is the results shown for the first week. Plots showing the disaggregation of the entire period for each appliance can be found in Appendix~\ref{APP:viewship}.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{billeder/Viewership.png}
\caption{Viewership in week one}
\label{fig:WHW}
\end{figure}

The figure shows the viewership, that tells how many in the city is watching TV at a given time. The green line is the actual viewership, where the blue is the viewership reported by the utility company. The results differed, as one would expect, since the disaggregation process in an environment with high \df{background consumption} is a hard task. The general trends that many are watching TV in the evening, and not that many in the morning, is still represented nicely. This illustrates that the general trends of the viewership is maintained in the disaggregated signal, and this is commonly what is important for this kind of costumer statistics. 

In Table~\ref{tab:CaseRes} is the average number of viewers in 3 hour timeslots shown. The results shown in the table is the average of all 6 weeks, split up in the different weekdays. 

\input{tables/TVCaseResTab}

This illustrates that the true trends in the information is maintained. The results indicates that it is to some degree possible to obtain user information from the disaggregated data. The TV is a relative hard appliance to detect, since there are many different types. The success in this experiment is partly due to the fact that the TV was some of the main consumers of the houses selected, and they had a pure Type-I behaviour.

At the current state of \ab{NILM} is the information only limited to a small amount of devices, since it is hard to detect other appliances than the top consumers in a household. One of the techniques shown to greatly improve the effectiveness of \ab{NILM} is to switch from the sub 1 Hz sampling range to a higher sampling rate in the kHz range. In the future there might be smart-meters capable of delivering information at high frequency or the \ab{NILM} techniques have advanced so it is possible to detect all appliances. This will greatly improve the possibility for the \dfs{service provider} to deliver very detailed user behavioural statistics. This information can be of interest for companies developing and selling electronics, since they are able to get accurate user reports about their products. 

\section{Chapter Summery}
This case is a very simple example of how the \dfs{service provider} can extend their product range to statistical information. This information could potentially also be used to extend their product range to the \dfs{customer}. One example of this could be a fridge surveillance system that would send warning messages to the resident if their fridge stopped working.

The cases focused around getting information from a TV. This information was used to extract some general statistical properties about a small city's viewing habits. This is done by using the \ab{FHMM} algorithm and a \df{norm filter}. The approach is relatively successful. This indicates that such approaches can be used to extend the \dfs{service provider} product range. All TV appliances was relatively large energy consumers in the house. This makes them easy to detect. For appliances that are small consumers it is unlikely that the approach will have as big an effect.