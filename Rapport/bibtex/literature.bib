


@book{RefWorks:7,
	author={ISO 19157:2013},
	year={2013-12-15},
	title={Geographic information -- Data quality},
	publisher={ISO},
	edition={1}
}

@book{RefWorks:5,
	author={ISO 8402:1994},
	year={1994-03-24},
	title={Quality management and quality assurance -- Vocabulary},
	publisher={ISO},
	edition={2}
}

@misc{RefWorks:2,
  author = 	 {Steve Kelling and Daniel Fink and Frank A. La Sorte and Alison Johnston and Nicholas E. Bruns and Wesley M. Hochachka},
  month = 	 {2015-10-27; 2015-11},
  title = 	 {Taking a ?Big Data? approach to data quality in a citizen science project},
  note = 	 {Type: Text; Â© The Author(s) 2015; Open AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.},
  abstract = 	 {Data from well-designed experiments provide the strongest evidence of causation in biodiversity studies. However, for many species the collection of these data is not scalable to the spatial and temporal extents required to understand patterns at the population level. Only data collected from citizen science projects can gather sufficient quantities of data, but data collected from volunteers are inherently noisy and heterogeneous. Here we describe a ?Big Data? approach to improve the data quality in eBird, a global citizen science project that gathers bird observations. First, eBird?s data submission design ensures that all data meet high standards of completeness and accuracy. Second, we take a ?sensor calibration? approach to measure individual variation in eBird participant?s ability to detect and identify birds. Third, we use species distribution models to fill in data gaps. Finally, we provide examples of novel analyses exploring population-level patterns in bird distributions.},
  keywords = 	 {Article},
  language = 	 {en}
}

@misc{RefWorks:4,
  author = 	 {M. Meijer and L. A. E. Vullings and J. D. Bulens and F. I. Rip and M. Boss and G. Hazeu and M. Storm},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {SPATIAL DATA QUALITY AND A WORKFLOW TOOL},
  note = 	 {Type: article; CC BY},
  abstract = 	 {Although by many perceived as important, spatial data quality has hardly ever been taken centre stage unless something went wrong due to bad quality. However, we think this is going to change soon. We are more and more relying on data driven processes and due to the increased availability of data, there is a choice in what data to use. How to make that choice? We think spatial data quality has potential as a selection criterion.

In this paper we focus on how a workflow tool can help the consumer as well as the producer to get a better understanding about which product characteristics are important. For this purpose, we have developed a framework in which we define different roles (consumer, producer and intermediary) and differentiate between product specifications and quality specifications. A number of requirements is stated that can be translated into quality elements. We used case studies to validate our framework. This framework is designed following the fitness for use principle. Also part of this framework is software that in some cases can help ascertain the quality of datasets.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@misc{RefWorks:3,
  author = 	 {N. Regnauld},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {GENERALISATION AND DATA QUALITY},
  note = 	 {Type: article; CC BY},
  abstract = 	 {The quality of spatial data has a massive impact on its usability. It is therefore critical to both the producer of the data and its users.In this paper we discuss the close links between data quality and the generalisation process. The quality of the source data has aneffect on how it can be generalised, and the generalisation process has an effect on the quality of the output data. Data qualitytherefore needs to be kept under control. We explain how this can be done before, during and after the generalisation process, usingthree of 1Spatial?s software products: 1Validate for assessing the conformance of a dataset against a set of rules, 1Integrate forautomatically fixing the data when non-conformances have been detected and 1Generalise for controlling the quality during thegeneralisation process. These tools are very effective at managing data that need to conform to a set of quality rules, the mainremaining challenge is to be able to define a set of quality rules that reflects the fitness of a dataset for a particular purpose.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@inproceedings{RefWorks:6,
	author={G. Pastorello and D. Agarwal and T. Samak and C. Poindexter and B. Faybishenko and D. Gunter and R. Hollowgrass and D. Papale and C. Trotta and A. Ribeca and E. Canfora},
	year={2014},
	title={Observational Data Patterns for Time Series Data Quality Assessment},
	booktitle={e-Science (e-Science), 2014 IEEE 10th International Conference on},
	volume={1},
	pages={271-278},
	note={ID: 1}
}
