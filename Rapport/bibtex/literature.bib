


@misc{RefWorks:7,
  author = 	 {ISO 19157:2013},
  year = 	 {2013-12-15},
  title = 	 {Geographic information -- Data quality}
}

@misc{RefWorks:5,
  author = 	 {ISO 8402:1994},
  year = 	 {1994-03-24},
  title = 	 {Quality management and quality assurance -- Vocabulary}
}

@misc{RefWorks:2,
  author = 	 {Steve Kelling and Daniel Fink and Frank A. La Sorte and Alison Johnston and Nicholas E. Bruns and Wesley M. Hochachka},
  month = 	 {2015-10-27; 2015-11},
  title = 	 {Taking a ?Big Data? approach to data quality in a citizen science project},
  note = 	 {Type: Text; Â© The Author(s) 2015; Open AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.},
  abstract = 	 {Data from well-designed experiments provide the strongest evidence of causation in biodiversity studies. However, for many species the collection of these data is not scalable to the spatial and temporal extents required to understand patterns at the population level. Only data collected from citizen science projects can gather sufficient quantities of data, but data collected from volunteers are inherently noisy and heterogeneous. Here we describe a ?Big Data? approach to improve the data quality in eBird, a global citizen science project that gathers bird observations. First, eBird?s data submission design ensures that all data meet high standards of completeness and accuracy. Second, we take a ?sensor calibration? approach to measure individual variation in eBird participant?s ability to detect and identify birds. Third, we use species distribution models to fill in data gaps. Finally, we provide examples of novel analyses exploring population-level patterns in bird distributions.},
  keywords = 	 {Article},
  language = 	 {en}
}

@misc{RefWorks:4,
  author = 	 {M. Meijer and L. A. E. Vullings and J. D. Bulens and F. I. Rip and M. Boss and G. Hazeu and M. Storm},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {SPATIAL DATA QUALITY AND A WORKFLOW TOOL},
  note = 	 {Type: article; CC BY},
  abstract = 	 {Although by many perceived as important, spatial data quality has hardly ever been taken centre stage unless something went wrong due to bad quality. However, we think this is going to change soon. We are more and more relying on data driven processes and due to the increased availability of data, there is a choice in what data to use. How to make that choice? We think spatial data quality has potential as a selection criterion.

In this paper we focus on how a workflow tool can help the consumer as well as the producer to get a better understanding about which product characteristics are important. For this purpose, we have developed a framework in which we define different roles (consumer, producer and intermediary) and differentiate between product specifications and quality specifications. A number of requirements is stated that can be translated into quality elements. We used case studies to validate our framework. This framework is designed following the fitness for use principle. Also part of this framework is software that in some cases can help ascertain the quality of datasets.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@misc{RefWorks:8,
  author = 	 {M. Meijer and L. A. E. Vullings and J. D. Bulens and F. I. Rip and M. Boss and G. Hazeu and M. Storm},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {SPATIAL DATA QUALITY AND A WORKFLOW TOOL},
  note = 	 {Type: article; CC BY},
  abstract = 	 {Although by many perceived as important, spatial data quality has hardly ever been taken centre stage unless something went wrong due to bad quality. However, we think this is going to change soon. We are more and more relying on data driven processes and due to the increased availability of data, there is a choice in what data to use. How to make that choice? We think spatial data quality has potential as a selection criterion.

In this paper we focus on how a workflow tool can help the consumer as well as the producer to get a better understanding about which product characteristics are important. For this purpose, we have developed a framework in which we define different roles (consumer, producer and intermediary) and differentiate between product specifications and quality specifications. A number of requirements is stated that can be translated into quality elements. We used case studies to validate our framework. This framework is designed following the fitness for use principle. Also part of this framework is software that in some cases can help ascertain the quality of datasets.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@misc{RefWorks:3,
  author = 	 {N. Regnauld},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {GENERALISATION AND DATA QUALITY},
  note = 	 {Type: article; CC BY},
  abstract = 	 {The quality of spatial data has a massive impact on its usability. It is therefore critical to both the producer of the data and its users.In this paper we discuss the close links between data quality and the generalisation process. The quality of the source data has aneffect on how it can be generalised, and the generalisation process has an effect on the quality of the output data. Data qualitytherefore needs to be kept under control. We explain how this can be done before, during and after the generalisation process, usingthree of 1Spatial?s software products: 1Validate for assessing the conformance of a dataset against a set of rules, 1Integrate forautomatically fixing the data when non-conformances have been detected and 1Generalise for controlling the quality during thegeneralisation process. These tools are very effective at managing data that need to conform to a set of quality rules, the mainremaining challenge is to be able to define a set of quality rules that reflects the fitness of a dataset for a particular purpose.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@inproceedings{RefWorks:6,
	author={G. Pastorello and D. Agarwal and T. Samak and C. Poindexter and B. Faybishenko and D. Gunter and R. Hollowgrass and D. Papale and C. Trotta and A. Ribeca and E. Canfora},
	year={2014},
	title={Observational Data Patterns for Time Series Data Quality Assessment},
	booktitle={e-Science (e-Science), 2014 IEEE 10th International Conference on},
	volume={1},
	pages={271-278},
	note={ID: 1}
}

@misc{RefWorks:9,
  author = 	 {Alexandra Simader and Bernhard Kluger and Nora Neumann and Christoph Bueschl and Marc Lemmens and Gerald Lirk and Rudolf Krska and Rainer Schuhmacher},
  month = 	 {2015-10-24},
  title = 	 {QCScreen: a software tool for data quality control in LC-HRMS based metabolomics},
  note = 	 {Type: Software; Copyright 2015 Simader et al.},
  abstract = 	 {Abstract Background Metabolomics experiments often comprise large numbers of biological samples resulting in huge amounts of data. This data needs to be inspected for plausibility before data evaluation to detect putative sources of error e.g. retention time or mass accuracy shifts. Especially in liquid chromatography-high resolution mass spectrometry (LC-HRMS) based metabolomics research, proper quality control checks (e.g. for precision, signal drifts or offsets) are crucial prerequisites to achieve reliable and comparable results within and across experimental measurement sequences. Software tools can support this process. Results The software tool QCScreen was developed to offer a quick and easy data quality check of LC-HRMS derived data. It allows a flexible investigation and comparison of basic quality-related parameters within user-defined target features and the possibility to automatically evaluate multiple sample types within or across different measurement sequences in a short time. It offers a user-friendly interface that allows an easy selection of processing steps and parameter settings. The generated results include a coloured overview plot of data quality across all analysed samples and targets and, in addition, detailed illustrations of the stability and precision of the chromatographic separation, the mass accuracy and the detector sensitivity. The use of QCScreen is demonstrated with experimental data from metabolomics experiments using selected standard compounds in pure solvent. The application of the software identified problematic features, samples and analytical parameters and suggested which data files or compounds required closer manual inspection. Conclusions QCScreen is an open source software tool which provides a useful basis for assessing the suitability of LC-HRMS data prior to time consuming, detailed data processing and subsequent statistical analysis. It accepts the generic mzXML format and thus can be used with many different LC-HRMS platforms to process both multiple quality control sample types as well as experimental samples in one or more measurement sequences.},
  keywords = 	 {QC; LC-HRMS; Retention time shift; Mass accuracy shift; mzXML},
  language = 	 {en}
}
