


@article{RefWorks:11,
	author={P. J. S. G. Ferreira},
	year={1994},
	title={Interpolation and the discrete Papoulis-Gerchberg algorithm},
	journal={IEEE Transactions on Signal Processing},
	volume={42},
	number={10},
	pages={2596-2606},
	note={Analyze the performance of an iterative algorithm, similar to the discrete Papoulis-Gerchberg algorithm, and which can be used to recover missing samples in...   An analysis of the performance of an iterative algorithm that can be used to recover missing samples in finite-length records of band-limited data.No...},
	abstract={Analyze the performance of an iterative algorithm, similar to the discrete Papoulis-Gerchberg algorithm, and which can be used to recover missing samples in finite-length records of band-limited data. No assumptions are made regarding the distribution of the missing samples, in contrast with the often studied extrapolation problem, in which the known samples are grouped together. Indeed, it is possible to regard the observed signal as a sampled version of the original one, and to interpret the reconstruction result studied as a sampling result. The authors show that the iterative algorithm converges if the density of the sampling set exceeds a certain minimum value which naturally increases with the bandwidth of the data. They give upper and lower bounds for the error as a function of the number of iterations, together with the signals for which the bounds are attained. Also, they analyze the effect of a relaxation constant present in the algorithm on the spectral radius of the iteration matrix. From this analysis they infer the optimum value of the relaxation constant. They also point out, among all sampling sets with the same density, those for which the convergence rate of the recovery algorithm is maximum or minimum. For low-pass signals it turns out that the best convergence rates result when the distances among the missing samples are a multiple of a certain integer. The worst convergence rates generally occur when the missing samples are contiguous.},
	keywords={Extrapolation; missing samples; signal processing; Convergence; spectral radius; iteration matrix; sampling set density; Bandwidth; error; iterative methods; band-limited data; reconstruction result; Iterative algorithms; Algorithm design and analysis; interpolation; discrete Papoulis-Gerchberg algorithm; Performance analysis; Image reconstruction; recovery algorithm; convergence of numerical methods; finite-length records; iterative algorithm; relaxation constant; convergence rate; Discrete Fourier transforms; Sampling methods; low-pass signals; Iterative methods (Mathematics); Research; Signal reconstruction/Mathematical models},
	isbn={1053-587X},
	language={English}
}

@misc{RefWorks:7,
  author = 	 {ISO 19157:2013},
  year = 	 {2013-12-15},
  title = 	 {Geographic information -- Data quality}
}

@misc{RefWorks:5,
  author = 	 {ISO 8402:1994},
  year = 	 {1994-03-24},
  title = 	 {Quality management and quality assurance -- Vocabulary}
}

@misc{RefWorks:2,
  author = 	 {Steve Kelling and Daniel Fink and Frank A. La Sorte and Alison Johnston and Nicholas E. Bruns and Wesley M. Hochachka},
  month = 	 {2015-10-27; 2015-11},
  title = 	 {Taking a ?Big Data? approach to data quality in a citizen science project},
  note = 	 {Type: Text; Â© The Author(s) 2015; Open AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.},
  abstract = 	 {Data from well-designed experiments provide the strongest evidence of causation in biodiversity studies. However, for many species the collection of these data is not scalable to the spatial and temporal extents required to understand patterns at the population level. Only data collected from citizen science projects can gather sufficient quantities of data, but data collected from volunteers are inherently noisy and heterogeneous. Here we describe a ?Big Data? approach to improve the data quality in eBird, a global citizen science project that gathers bird observations. First, eBird?s data submission design ensures that all data meet high standards of completeness and accuracy. Second, we take a ?sensor calibration? approach to measure individual variation in eBird participant?s ability to detect and identify birds. Third, we use species distribution models to fill in data gaps. Finally, we provide examples of novel analyses exploring population-level patterns in bird distributions.},
  keywords = 	 {Article},
  language = 	 {en}
}

@article{RefWorks:15,
	author={D. Kondrashov and M. Ghil},
	year={2006},
	title={Spatio-temporal filling of missing points in geophysical data sets},
	journal={Nonlinear Processes in Geophysics},
	volume={13},
	number={2},
	pages={151-159},
	note={The majority of data sets in the geosciences are obtained from observations and measurements of natural systems, rather than in the laboratory. These data... The majority of data sets in the geosciences are obtained from observations and measurements of natural systems, rather than in the laboratory. These data sets...},
	abstract={The majority of data sets in the geosciences are obtained from observations and measurements of natural systems, rather than in the laboratory. These data sets are often full of gaps, due to to the conditions under which the measurements are made. Missing data give rise to various problems, for example in spectral estimation or in specifying boundary conditions for numerical models. Here we use Singular Spectrum Analysis (SSA) to fill the gaps in several types of data sets. For a univariate record, our procedure uses only temporal correlations in the data to fill in the missing points. For a multivariate record, multi-channel SSA (M-SSA) takes advantage of both spatial and temporal correlations. We iteratively produce estimates of missing data points, which are then used to compute a self-consistent lag-covariance matrix; cross-validation allows us to optimize the window width and number of dominant SSA or M-SSA modes to fill the gaps. The optimal parameters of our procedure depend on the distribution in time (and space) of the missing data, as well as on the variance distribution between oscillatory modes and noise. The algorithm is demonstrated on synthetic examples, as well as on data sets from oceanography, hydrology, atmospheric sciences, and space physics: global sea-surface temperature, flood-water records of the Nile River, the Southern Oscillation Index (SOI), and satellite observations of relativistic electrons.[PUBLICATION ABSTRACT]},
	keywords={Data bases; Spectrum analysis; Geophysics; Physics; Geophysics. Cosmic physics; QC1-999; Geology; QE1-996.5; QC801-809; Science},
	isbn={1023-5809 1607-7946},
	language={English}
}

@misc{RefWorks:4,
  author = 	 {M. Meijer and L. A. E. Vullings and J. D. Bulens and F. I. Rip and M. Boss and G. Hazeu and M. Storm},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {SPATIAL DATA QUALITY AND A WORKFLOW TOOL},
  note = 	 {Type: article; CC BY},
  abstract = 	 {Although by many perceived as important, spatial data quality has hardly ever been taken centre stage unless something went wrong due to bad quality. However, we think this is going to change soon. We are more and more relying on data driven processes and due to the increased availability of data, there is a choice in what data to use. How to make that choice? We think spatial data quality has potential as a selection criterion.

In this paper we focus on how a workflow tool can help the consumer as well as the producer to get a better understanding about which product characteristics are important. For this purpose, we have developed a framework in which we define different roles (consumer, producer and intermediary) and differentiate between product specifications and quality specifications. A number of requirements is stated that can be translated into quality elements. We used case studies to validate our framework. This framework is designed following the fitness for use principle. Also part of this framework is software that in some cases can help ascertain the quality of datasets.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@misc{RefWorks:8,
  author = 	 {M. Meijer and L. A. E. Vullings and J. D. Bulens and F. I. Rip and M. Boss and G. Hazeu and M. Storm},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {SPATIAL DATA QUALITY AND A WORKFLOW TOOL},
  note = 	 {Type: article; CC BY},
  abstract = 	 {Although by many perceived as important, spatial data quality has hardly ever been taken centre stage unless something went wrong due to bad quality. However, we think this is going to change soon. We are more and more relying on data driven processes and due to the increased availability of data, there is a choice in what data to use. How to make that choice? We think spatial data quality has potential as a selection criterion.

In this paper we focus on how a workflow tool can help the consumer as well as the producer to get a better understanding about which product characteristics are important. For this purpose, we have developed a framework in which we define different roles (consumer, producer and intermediary) and differentiate between product specifications and quality specifications. A number of requirements is stated that can be translated into quality elements. We used case studies to validate our framework. This framework is designed following the fitness for use principle. Also part of this framework is software that in some cases can help ascertain the quality of datasets.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@book{RefWorks:10,
	author={Dimitris G. Manolakis and Vinay K. Ingle},
	year={2011},
	title={Applied digital signal processing : theory and practice},
	publisher={Cambridge University Press},
	address={New York},
	pages={991},
	keywords={elektronisk signalbehandling; Signal processing - Digital techniques; digital signalbehandling; digital signal processing; DSP; elektriske kredslÃ¸b; LTI systems; discrete-time-systems; multirate signal processing; electronic circuits},
	isbn={9780521110020},
	language={eng}
}

@article{RefWorks:13,
	author={Manuel Marques and Alexandre Neves and JorgeS Marques and JoÃÂ£o Sanches},
	year={2006},
	title={The Papoulis-Gerchberg Algorithm with Unknown Signal Bandwidth},
	volume={4141},
	pages={436-445},
	isbn={978-3-540-44891-4},
	language={English},
	url={http://dx.doi.org/10.1007/11867586_41}
}

@misc{RefWorks:16,
  author = 	 {A. Moghtaderi and P. Borgnat and P. Flandrin},
  year = 	 {2012},
  title = 	 {Gap-filling by the empirical mode decomposition},
  journal = 	 {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = 	 {3821-3824},
  note = 	 {We propose a novel gap-filling technique, based on the empirical mode decomposition (EMD). The idea is that a signal with missing data can be decomposed into a...},
  abstract = 	 {We propose a novel gap-filling technique, based on the empirical mode decomposition (EMD). The idea is that a signal with missing data can be decomposed into a set of intrinsic mode functions (IMFs) with missing data. Filling the gaps in each IMF should be easier than filling the gaps in the original signal. This is because each IMF varies much more slowly than the original signal, and also because the IMFs are known to have useful regularity properties. We demonstrate the performance of our technique on environmental pollutant data.},
  keywords = 	 {empirical mode decomposition; gap-filling technique; Signal processing algorithms; Spectral analysis; Zirconium; signal processing; signal restoration; Interpolation; Indexes; Signal reconstruction; missing data decomposition; environmental pollutant data; EMD; Splines (mathematics); IMF},
  isbn = 	 {1520-6149; 1467300454 9781467300452},
  language = 	 {English}
}

@misc{RefWorks:3,
  author = 	 {N. Regnauld},
  month = 	 {2015-08-01T00:00:00Z},
  title = 	 {GENERALISATION AND DATA QUALITY},
  note = 	 {Type: article; CC BY},
  abstract = 	 {The quality of spatial data has a massive impact on its usability. It is therefore critical to both the producer of the data and its users.In this paper we discuss the close links between data quality and the generalisation process. The quality of the source data has aneffect on how it can be generalised, and the generalisation process has an effect on the quality of the output data. Data qualitytherefore needs to be kept under control. We explain how this can be done before, during and after the generalisation process, usingthree of 1Spatial?s software products: 1Validate for assessing the conformance of a dataset against a set of rules, 1Integrate forautomatically fixing the data when non-conformances have been detected and 1Generalise for controlling the quality during thegeneralisation process. These tools are very effective at managing data that need to conform to a set of quality rules, the mainremaining challenge is to be able to define a set of quality rules that reflects the fitness of a dataset for a particular purpose.},
  keywords = 	 {Technology; T; Engineering (General); Civil engineering (General); TA1-2040; Applied optics; Photonics; TA1501-1820},
  language = 	 {EN}
}

@inproceedings{RefWorks:6,
	author={G. Pastorello and D. Agarwal and T. Samak and C. Poindexter and B. Faybishenko and D. Gunter and R. Hollowgrass and D. Papale and C. Trotta and A. Ribeca and E. Canfora},
	year={2014},
	title={Observational Data Patterns for Time Series Data Quality Assessment},
	booktitle={e-Science (e-Science), 2014 IEEE 10th International Conference on},
	volume={1},
	pages={271-278},
	note={ID: 1}
}

@misc{RefWorks:9,
  author = 	 {Alexandra Simader and Bernhard Kluger and Nora Neumann and Christoph Bueschl and Marc Lemmens and Gerald Lirk and Rudolf Krska and Rainer Schuhmacher},
  month = 	 {2015-10-24},
  title = 	 {QCScreen: a software tool for data quality control in LC-HRMS based metabolomics},
  note = 	 {Type: Software; Copyright 2015 Simader et al.},
  abstract = 	 {Abstract Background Metabolomics experiments often comprise large numbers of biological samples resulting in huge amounts of data. This data needs to be inspected for plausibility before data evaluation to detect putative sources of error e.g. retention time or mass accuracy shifts. Especially in liquid chromatography-high resolution mass spectrometry (LC-HRMS) based metabolomics research, proper quality control checks (e.g. for precision, signal drifts or offsets) are crucial prerequisites to achieve reliable and comparable results within and across experimental measurement sequences. Software tools can support this process. Results The software tool QCScreen was developed to offer a quick and easy data quality check of LC-HRMS derived data. It allows a flexible investigation and comparison of basic quality-related parameters within user-defined target features and the possibility to automatically evaluate multiple sample types within or across different measurement sequences in a short time. It offers a user-friendly interface that allows an easy selection of processing steps and parameter settings. The generated results include a coloured overview plot of data quality across all analysed samples and targets and, in addition, detailed illustrations of the stability and precision of the chromatographic separation, the mass accuracy and the detector sensitivity. The use of QCScreen is demonstrated with experimental data from metabolomics experiments using selected standard compounds in pure solvent. The application of the software identified problematic features, samples and analytical parameters and suggested which data files or compounds required closer manual inspection. Conclusions QCScreen is an open source software tool which provides a useful basis for assessing the suitability of LC-HRMS data prior to time consuming, detailed data processing and subsequent statistical analysis. It accepts the generic mzXML format and thus can be used with many different LC-HRMS platforms to process both multiple quality control sample types as well as experimental samples in one or more measurement sequences.},
  keywords = 	 {QC; LC-HRMS; Retention time shift; Mass accuracy shift; mzXML},
  language = 	 {en}
}

@article{RefWorks:14,
	author={D. J. Thomson and L. J. Lanzerotti and C. G. Maclennan},
	year={2001},
	title={Interplanetary magnetic field: Statistical properties and discrete modes},
	journal={Journal of Geophysical Research},
	volume={106},
	number={A8},
	pages={15941-15962},
	isbn={0148-0227},
	language={English}
}
